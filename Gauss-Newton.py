import numpy as np
import matplotlib.pyplot as plt

# Access the columns of the DataFrame

X1=[0.548813504,0.715189366,0.602763376,0.544883183,0.423654799,0.645894113,0.437587211,0.891773001,0.963662761,0.383441519,0.791725038,0.52889492,0.568044561,0.925596638,
0.071036058,0.0871293,0.020218397,0.832619846,0.778156751,0.870012148,0.978618342,0.799158564,0.461479362,0.780529176,0.118274426,0.639921021,0.143353287,0.944668917,
0.521848322,0.41466194,0.264555612,0.774233689,0.456150332,0.568433949,0.0187898,0.617635497,0.612095723,0.616933997,0.943748079,0.681820299,
0.359507901,0.437031954,0.697631196,0.060225472,0.666766715,0.67063787,0.210382561,0.128926298,0.315428351,0.363710771,0.57019677,0.438601513,0.988373838,0.102044811,0.208876756,0.161309518,0.653108325,
0.253291603,0.466310773,0.244425592,0.158969584,0.110375141,0.656329589,0.138182951,0.196582362,0.368725171,0.82099323,0.097101276,0.837944907,0.096098408,0.976459465,0.468651202,0.976761088,
0.60484552,0.739263579,0.039187792,0.282806963,0.120196561,0.296140198,0.118727719,0.317983179,0.414262995,0.064147496,0.692472119,0.566601454,0.265389491,0.523248053,0.093940511,0.575946496,
0.929296198,0.318568952,0.66741038,0.131797862,0.716327204,0.289406093,0.183191362,0.586512935,0.020107546,0.828940029,0.004695476,0.677816537,0.270007973,0.735194022,0.962188545,0.248753144,0.576157334,
0.592041931,0.572251906,0.223081633,0.952749012,0.447125379,0.846408672,0.699479275,0.297436951,0.81379782,0.396505741,0.881103197,0.581272873,0.881735362,0.69253159,
0.72525428,0.501324382,0.956083635,0.643990199,0.423855049,0.606393214,0.019193198,0.301574817,0.660173537,0.290077607,0.618015429,0.428768701,0.135474064,0.298282326,0.569964911,0.590872761,
0.574325249,0.65320082,0.65210327,0.431418435,0.896546596,0.36756187,0.435864925,0.891923355,0.806193989,0.703888584,0.100226887,0.919482614,0.7142413,0.998847007,0.149448305,0.868126057,
0.162492935,0.615559564,0.123819983,0.848008229,0.807318959,0.569100739,0.407183297,0.069166995,0.697428773,0.453542683,0.722055599,0.866382326,0.975521505,0.855803342,0.011714084,0.359978064,
0.729990562,0.171629677,0.521036606,0.054337988,0.199996525,0.018521794,0.793697703,0.223924688,0.345351681,0.928081293,0.704414402,0.03183893,0.164694156,0.621478401,0.577228589,0.237892821,
0.934213998,0.613965956,0.535632803,0.589909976,0.73012203,0.311944995,0.398221062,0.209843749,0.186193006,0.94437239,0.739550795,0.490458809,0.227414628,0.254356482,0.05802916,0.434416626]

X2 =[0.311795882,0.696343489,0.377751839,0.179603678,0.024678728,0.067249631,0.679392773,0.453696845,0.536579211,0.896671293,0.990338947,0.216896984,0.663078203,0.263322377,0.020650999,0.758378654,
0.320017151,0.383463894,0.588317114,0.831048455,0.628981844,0.872650655,0.273542035,0.798046834,0.185635944,0.952791657,
0.687488276,0.215507677,0.94737059,0.730855807,0.253941643,0.213311977,0.518200714,0.025662718,0.207470075,0.424685469,0.37416998,0.463575424,0.277628706,0.586784346,0.863855606,
0.117531856,0.517379107,0.132068106,0.716859681,0.396059703,0.565421312,0.183279836,0.144847759,0.488056281,0.355612738,0.940431945,0.765325254,0.74866362,0.90371974,0.083422435,0.55219247,
0.584476069,0.961936379,0.292147527,0.24082878,0.100293942,0.01642963,0.929529317,0.669916547,0.785152912,0.281730106,0.586410166,0.063955266,0.485627596,0.97749514,0.876505245,0.338158952,0.961570155,
0.231701626,0.949318822,0.941377705,0.799202587,0.630447937,0.874287967,0.293020285,0.848943555,0.617876692,0.013236858,0.347233518,0.148140861,0.98182939,0.478370307,0.497391365,0.639472516,0.368584606,0.136900272,0.822117733,0.189847912,0.511318983,
0.224317029,0.097844484,0.862191517,0.972919489,0.960834658,0.906555499,0.774047333,0.333145152,0.08110139,0.407241171,0.232234142,0.132487635,0.053427182,0.725594364,0.011427459,0.770580749,
0.146946645,0.079522083,0.089603034,0.672047807,0.24536721,0.420539467,0.557368791,0.860551174,0.727044263,0.270327905,0.131482799,0.05537432,0.301598634,0.262118149,0.456140567,0.683281336,
0.695625446,0.283518847,0.379926956,0.181150962,0.788545512,0.056848076,0.696997242,0.778695396,0.777407562,0.259422564,0.373813138,0.587599635,0.272821902,0.370852799,0.19705428,0.459855884,
0.044612301,0.799795885,0.076956447,0.518835149,0.3068101,0.577542949,0.959433341,0.645570244,0.035362436,0.43040244,0.510016852,0.536177495,0.681392511,0.277596098,0.128860565,0.392675677,0.956405723,0.187130892,0.903983955,0.54380595,0.456911422,0.88204141,0.458603962,0.724167637,
0.399025322,0.904044393,0.69002502,0.699622054,0.327720402,0.756778643,0.636061055,0.240020273,0.160538822,0.796391475,
0.959166603,0.458138827,0.590984165,0.857722644,0.457223453,0.951874477,0.575751162,0.820767121,0.908843718,0.815523819,0.159414463,0.628898439,0.398434259,0.062712952,0.424032252,0.258684067,0.849038308,0.033304627,0.958982722,0.355368848,0.35670689,0.016328503,0.185232325]

y =[3.206457942,4.47951584,3.30535688,2.89801335,1.639639332,2.310867682,3.817132791,4.168224433,4.526520292,4.385175119,5.539060318,2.681654474,3.945422634,3.43791216,1.272654204,3.22666579,
1.711149433,3.737558311,4.281848089,5.797350537,4.668007146,5.452084276,2.930381913,4.657962616,1.986769929,4.842246854,2.684378344,3.687440747,4.446835769,4.134624916,2.119933428,3.60329101,
3.734030156,2.100509601,1.488030424,3.20580805,3.236470728,3.554505393,3.62920889,4.163169601,4.455212993,2.31407309,3.756363732,1.157207394,4.825245436,3.357092552,2.953955658,1.677394776,
1.604632592,3.072096883,3.087317801,4.853588437,5.447337725,3.451023203,4.361874825,1.657877588,3.958873533,3.300243454,4.770767308,2.266581386,1.973492123,1.239629276,2.432058494,3.81667295,
3.613322179,4.030544432,3.499550522,3.076892244,3.02858423,2.256423751,5.83367843,4.786862867,3.543472577,5.191221622,2.609740981,3.670705341,4.399404677,3.223822109,3.237246521,3.492360586,2.927060945,4.416413594,3.123747638,2.368986037,3.086545525,1.571083018,4.919024936,2.43261889,
3.858548069,5.062285411,3.109536402,2.95865956,3.58028544,2.723223897,3.304434929,
2.128407015,2.02442471,3.715660093,5.780268481,3.906626324,5.029036154,3.660245823,3.108189825,3.367755748,2.64195119,2.79065043,3.014727064,2.475910634,3.715652608,2.97529585,4.585991718,
3.563554608,2.869901076,2.009239152,4.120088294,2.56004359,3.991298056,3.858140427,5.580885767,3.881276676,3.119164262,2.46457325,2.961578844,2.838549774,2.851305417,3.650426105,2.845454261,3.768730271,3.376300043,2.721259244,2.979624944,4.242738983,1.342685112,3.397701248,4.454533317,
4.562541443,3.145876381,3.399064186,4.181359347,2.440149575,3.7099943,2.298689256,2.987640386,3.122745573,5.127558214,2.708420449,2.841685252,4.264656416,4.043895399,5.325633714,3.285432392,2.829688537,2.486813427,3.516462221,2.746375069,4.785528598,3.321722035,
3.127896594,2.752267529,3.809271819,2.384095212,4.681908334,3.571427392,3.968635258,5.528249607,3.909986579,3.630649248,3.165630692,5.501898523,3.19272971,4.423087891,2.215837418,3.863180465,3.202586462,3.080265415,1.823386438,4.295526788,5.069757623,
4.161577306,2.974913371,3.891130255,3.669754077,4.752596787,3.115553288,5.605800443,5.27896856,5.191893075,2.639582176,4.182301135,2.690634275,1.730070512,2.672320565,2.24412132,5.427299135,2.853102181,4.799311832,2.434073138,2.433516516,0.756885197,2.032588296]

# Now you can use X1 , X2 , and y 
#Question 1
#%%Question B&C Implement the gradient descent method
print("\n################################")
print("QB and C TEST OUTPUT:\n")

#Initialise
w1,w2,b = 0,0,0
iterations = 100
t = 0.1

# Normalize the f e at u r e s and t a r g e t
X1 = (X1 - np.mean(X1 ) ) / np.std(X1 )
X2 = (X2 - np.mean(X2 ) ) / np.std(X2 )
y = ( y -  np.mean(y ) ) /  np.std(y  )

def gradient_descent(X1,X2,y,w1,w2,b,t,iterations):
    m = iterations
    for _ in range(m):
        #Calculate the predicted values
        h = w1*X1 + w2*X2 + b
        
        #W1,W2,b
        w1 -= t*1/(m) * np.sum((h - y) * X1)
        w2 -= t*1/(m) * np.sum((h - y) * X2)
        b  -= t*1/(m) * np.sum( h - y)
        
    return w1,w2,b

def plot_B_R(X1, X2, y, w1, w2, b, t, iterations):
    
    w1, w2, b = gradient_descent(X1, X2, y, w1, w2, b, t, iterations)

    # Creating figure
    fig = plt.figure(figsize=(16, 9))
    ax = fig.add_subplot(111, projection='3d')

    # Scatter plot for data points
    ax.scatter3D(X1, X2, y, marker='o', label='$(x_1, x_2)$')

    # Decision boundary
    x1_range = np.linspace(min(X1), max(X1), 100)
    x2_range = np.linspace(min(X2), max(X2), 100)
    x1_vals, x2_vals = np.meshgrid(x1_range, x2_range)
    y_vals = w1 * x1_vals + w2 * x2_vals + b
    ax.plot_surface(x1_vals, x2_vals, y_vals, alpha=0.3, cmap='gist_heat',label='Decision Boundary')

    ax.set_xlabel('$x_1-axis$', fontweight='bold')
    ax.set_ylabel('$x_2-axis$', fontweight='bold')
    ax.set_zlabel('Y-axis', fontweight='bold')
    
    plt.show()
    

def gradient_descent_cost(X1,X2,y,w1,w2,b,t,iterations):
    cost = []
    m = iterations
    for _ in range(m):
        #Calculate the predicted values
        h = w1*X1 + w2*X2 + b
        
        #W1,W2,b
        w1 -= t*1/(m) * np.sum((h - y) * X1)
        w2 -= t*1/(m) * np.sum((h - y) * X2)
        b  -= t*1/(m) * np.sum( h - y)
        
        #Cost
        MSE_cost = (1/(2*m) )* np.sum((h-y)**2) 
        cost.append(MSE_cost)

    return w1, w2, b, cost

def cost_v_iteration(X1,X2,y,w1,w2,b,iterations,t):
    fig , ax = plt.subplots()
    
    for t_i in t:
        w1, w2, b, cost = gradient_descent_cost(X1,X2,y,w1,w2,b,t_i,iterations)
        ax.plot(range(0,iterations),cost,label = f'Step size = {t_i}')
    ax.set_yscale('log')
    ax.set_xlabel('Iterations')
    ax.set_ylabel('Cost in log scale')
    ax.legend()
    plt.show()

################

#Model
w1, w2, b = gradient_descent(X1,X2,y,w1,w2,b,t,iterations)
# Print the learned weights
print('w1:', w1)
print('w2:', w2)
print('b:', b)

print("\n################################")
print("QD TEST OUTPUT:\n")
X1 = (X1 - np.mean(X1 ) ) / np.std(X1 )
X2 = (X2 - np.mean(X2 ) ) / np.std(X2 )
y = ( y -  np.mean(y ) ) /  np.std(y  )
plot_B_R(X1, X2, y, w1, w2, b, t, iterations)


print("\n################################")
print("QE TEST OUTPUT:\n")
X1 = (X1 - np.mean(X1 ) ) / np.std(X1 )
X2 = (X2 - np.mean(X2 ) ) / np.std(X2 )
y = ( y -  np.mean(y ) ) /  np.std(y  )
w1,w2,b = 0,0,0
iterations =100
t = [0.01, 0.1, 0.9]
cost_v_iteration(X1,X2,y,w1,w2,b,iterations,t)


print("\n################################")
print("QE TEST OUTPUT:\n")
X1 = (X1 - np.mean(X1 ) ) / np.std(X1 )
X2 = (X2 - np.mean(X2 ) ) / np.std(X2 )
y = ( y -  np.mean(y ) ) /  np.std(y  )
w1,w2,b = 0,0,0
iterations =100
t = [1.9,1.92]
cost_v_iteration(X1,X2,y,w1,w2,b,iterations,t)

#%%Question 2

# Data
t = np.arange(1,12+1)
temp = np.array([7,8,10,13,17,19,22,21,18,14,10,8])

# Initial guess
x0 = np.array([5,1,0,10])
TOL = 10**(-6)
n = 1000
#QC
#Function f
def f(x,t):
    #t is time represented in months 1-12
    return x[0]*np.sin(x[1]*t + x[2]) + x[3]

#Jacobian
def Jocobian(x,t):
    Jocobian = np.zeros((len(t),len(x)))
    
    for k in range(0,len(t)):
        Jocobian[k,0] =           np.sin(t[k]*x[1] +x[2])
        Jocobian[k,1] = t[k]*x[0]*np.cos(t[k]*x[1] +x[2]) 
        Jocobian[k,2] =      x[0]*np.cos(t[k]*x[1] +x[2]) 
        Jocobian[k,3] = 1
    return Jocobian

def gradient(x,temp,t):
    F = f(x,t) - temp
    J = Jocobian(x,t)
    grad = (J.T)@F
    return grad

def Hessian(x,t):
    J = Jocobian(x,t)
    Hessian = (J.T)@J
    return Hessian
#Gaus-newtton and the Damping method
def gauss_newtton_or_Damp(x0,temp,t,n,TOL,Damp):
    x = x0.copy()
    for i in range(n):
        grad = gradient(x,temp,t)
        Hesh = Hessian(x, t)
        directional_deriv = np.linalg.solve(Hesh,-grad)
        
#damping
        if Damp == True:
            x = x + directional_deriv * min(1, 1 / np.linalg.norm(grad))
#Normal Gauss-Newtton
        else:
            x = x + directional_deriv
#Tolerance
        if np.linalg.norm(grad) < TOL:
            print(f'Function converged in {i+1} iterations')
            break
    return x 
 
print("\n################################")
print("The Gause-N method gives:", gauss_newtton_or_Damp(x0,temp,t,n,TOL,Damp=False))
print("\n################################")
print("The Damping Gause-N method gives:", gauss_newtton_or_Damp(x0,temp,t,n,TOL,Damp=True))
print("\n################################")
#%%

print("QF TEST OUTPUT:\n")
X1 = (X1 - np.mean(X1 ) ) / np.std(X1 )
X2 = (X2 - np.mean(X2 ) ) / np.std(X2 )
y = ( y -  np.mean(y ) ) /  np.std(y  )
w1,w2,b = 0,0,0
iterations =100
t = [1.9,1.92]
cost_v_iteration(X1,X2,y,w1,w2,b,iterations,t)
